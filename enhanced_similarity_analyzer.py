"""
Enhanced Similarity Analyzer
=====================================
é«˜ç´šç›¸ä¼¼æ€§åˆ†æå™¨ - å¯¦ç¾DTWç®—æ³•å’Œå¤šç¶­åº¦ç›¸ä¼¼æ€§åˆ†æ

ä¸»è¦åŠŸèƒ½:
1. Dynamic Time Warping (DTW) ç®—æ³• - è™•ç†æ™‚é–“è»¸æ‹‰ä¼¸/å£“ç¸®çš„ç›¸ä¼¼æ¨¡å¼
2. å¤šç¶­åº¦ç›¸ä¼¼æ€§åˆ†æ - åƒ¹æ ¼+æˆäº¤é‡+æ³¢å‹•ç‡çš„è¤‡åˆæ¨¡å¼åŒ¹é…
3. åƒ¹é‡é—œä¿‚åˆ†æ - è­˜åˆ¥"åƒ¹è·Œé‡ç¸®ã€åƒ¹æ¼²é‡å¢"ç­‰ç¶“å…¸æ¨¡å¼
4. é€²éšç›¸ä¼¼åº¦ç®—æ³• - è¶…è¶Šå‚³çµ±æ­å¼è·é›¢å’Œç›¸é—œä¿‚æ•¸

ä½œè€…: Claude AI Assistant
ç‰ˆæœ¬: 1.0
æ—¥æœŸ: 2025-07-03
"""

import os
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Union
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from scipy.spatial.distance import euclidean
from scipy.stats import pearsonr, spearmanr
from dtaidistance import dtw
from dtaidistance.dtw import distance as dtw_distance
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings('ignore')

# å°å…¥ç¾æœ‰æ¨¡å¡Š
from src.downloader import CryptoDownloader
from src.common import DataNormalizer, format_dt_with_tz, TrendAnalysisConfig

class EnhancedSimilarityAnalyzer:
    """å¢å¼·ç‰ˆç›¸ä¼¼æ€§åˆ†æå™¨"""
    
    def __init__(self, config: TrendAnalysisConfig = None):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.config = config or TrendAnalysisConfig()
        self.downloader = CryptoDownloader()
        self.scaler = StandardScaler()
        self.min_max_scaler = MinMaxScaler()
        
        # DTWåƒæ•¸
        self.dtw_window_ratio = 0.15  # DTWçª—å£æ¯”ä¾‹
        self.dtw_step_pattern = "symmetric2"  # DTWæ­¥é€²æ¨¡å¼
        self.dtw_distance_only = False  # æ˜¯å¦åªè¨ˆç®—è·é›¢
        
        # ç›¸ä¼¼æ€§é–¾å€¼
        self.similarity_thresholds = {
            'high': 0.8,      # é«˜ç›¸ä¼¼æ€§
            'medium': 0.6,    # ä¸­ç­‰ç›¸ä¼¼æ€§
            'low': 0.4        # ä½ç›¸ä¼¼æ€§
        }
        
        print("ğŸš€ å¢å¼·ç‰ˆç›¸ä¼¼æ€§åˆ†æå™¨å·²åˆå§‹åŒ–")
        print(f"   DTWçª—å£æ¯”ä¾‹: {self.dtw_window_ratio}")
        print(f"   ç›¸ä¼¼æ€§é–¾å€¼: {self.similarity_thresholds}")

    def calculate_dtw_similarity(self, series1: np.ndarray, series2: np.ndarray, 
                                window_ratio: float = None) -> Dict[str, float]:
        """
        è¨ˆç®—DTWç›¸ä¼¼æ€§
        
        Args:
            series1: ç¬¬ä¸€å€‹æ™‚é–“åºåˆ—
            series2: ç¬¬äºŒå€‹æ™‚é–“åºåˆ—
            window_ratio: DTWçª—å£æ¯”ä¾‹
            
        Returns:
            åŒ…å«DTWè·é›¢å’Œç›¸ä¼¼æ€§åˆ†æ•¸çš„å­—å…¸
        """
        if window_ratio is None:
            window_ratio = self.dtw_window_ratio
            
        try:
            # è¨ˆç®—DTWçª—å£å¤§å°
            window_size = int(max(len(series1), len(series2)) * window_ratio)
            
            # è¨ˆç®—DTWè·é›¢
            dtw_dist = dtw_distance(series1, series2, window=window_size)
            
            # è½‰æ›ç‚ºç›¸ä¼¼æ€§åˆ†æ•¸ (0-1ï¼Œ1ç‚ºæœ€ç›¸ä¼¼)
            similarity = 1 / (1 + dtw_dist)
            
            return {
                'dtw_distance': dtw_dist,
                'dtw_similarity': similarity,
                'window_size': window_size
            }
            
        except Exception as e:
            print(f"DTWè¨ˆç®—éŒ¯èª¤: {e}")
            return {
                'dtw_distance': float('inf'),
                'dtw_similarity': 0.0,
                'window_size': 0
            }

    def calculate_multidimensional_similarity(self, data1: pd.DataFrame, data2: pd.DataFrame) -> Dict[str, float]:
        """
        è¨ˆç®—å¤šç¶­åº¦ç›¸ä¼¼æ€§ (åƒ¹æ ¼+æˆäº¤é‡+æ³¢å‹•ç‡)
        
        Args:
            data1: ç¬¬ä¸€å€‹æ•¸æ“šé›† (åŒ…å«OHLCV)
            data2: ç¬¬äºŒå€‹æ•¸æ“šé›† (åŒ…å«OHLCV)
            
        Returns:
            å¤šç¶­åº¦ç›¸ä¼¼æ€§åˆ†æçµæœ
        """
        results = {}
        
        # ç¢ºä¿æ•¸æ“šé•·åº¦ä¸€è‡´
        min_len = min(len(data1), len(data2))
        if min_len < 10:
            return {'error': 'Data too short for analysis'}
        
        df1 = data1.tail(min_len).copy()
        df2 = data2.tail(min_len).copy()
        
        # 1. åƒ¹æ ¼ç›¸ä¼¼æ€§ (ä½¿ç”¨DTW)
        price1 = df1['Close'].values
        price2 = df2['Close'].values
        
        # æ¨™æº–åŒ–åƒ¹æ ¼
        price1_norm = self.normalize_series(price1)
        price2_norm = self.normalize_series(price2)
        
        price_similarity = self.calculate_dtw_similarity(price1_norm, price2_norm)
        results['price_dtw_similarity'] = price_similarity['dtw_similarity']
        results['price_dtw_distance'] = price_similarity['dtw_distance']
        
        # 2. æˆäº¤é‡ç›¸ä¼¼æ€§
        if 'Volume' in df1.columns and 'Volume' in df2.columns:
            volume1 = df1['Volume'].values
            volume2 = df2['Volume'].values
            
            # æ¨™æº–åŒ–æˆäº¤é‡
            volume1_norm = self.normalize_series(volume1)
            volume2_norm = self.normalize_series(volume2)
            
            volume_similarity = self.calculate_dtw_similarity(volume1_norm, volume2_norm)
            results['volume_dtw_similarity'] = volume_similarity['dtw_similarity']
            results['volume_dtw_distance'] = volume_similarity['dtw_distance']
        else:
            results['volume_dtw_similarity'] = 0.0
            results['volume_dtw_distance'] = float('inf')
        
        # 3. æ³¢å‹•ç‡ç›¸ä¼¼æ€§
        volatility1 = self.calculate_volatility(df1)
        volatility2 = self.calculate_volatility(df2)
        
        volatility_similarity = self.calculate_dtw_similarity(volatility1, volatility2)
        results['volatility_dtw_similarity'] = volatility_similarity['dtw_similarity']
        results['volatility_dtw_distance'] = volatility_similarity['dtw_distance']
        
        # 4. åƒ¹é‡é—œä¿‚ç›¸ä¼¼æ€§
        price_volume_relationship1 = self.calculate_price_volume_relationship(df1)
        price_volume_relationship2 = self.calculate_price_volume_relationship(df2)
        
        if price_volume_relationship1 is not None and price_volume_relationship2 is not None:
            pv_similarity = self.calculate_dtw_similarity(price_volume_relationship1, price_volume_relationship2)
            results['price_volume_dtw_similarity'] = pv_similarity['dtw_similarity']
            results['price_volume_dtw_distance'] = pv_similarity['dtw_distance']
        else:
            results['price_volume_dtw_similarity'] = 0.0
            results['price_volume_dtw_distance'] = float('inf')
        
        # 5. ç¶œåˆç›¸ä¼¼æ€§åˆ†æ•¸ (åŠ æ¬Šå¹³å‡)
        weights = {
            'price': 0.4,
            'volume': 0.2,
            'volatility': 0.2,
            'price_volume': 0.2
        }
        
        composite_similarity = (
            results['price_dtw_similarity'] * weights['price'] +
            results['volume_dtw_similarity'] * weights['volume'] +
            results['volatility_dtw_similarity'] * weights['volatility'] +
            results['price_volume_dtw_similarity'] * weights['price_volume']
        )
        
        results['composite_similarity'] = composite_similarity
        results['similarity_level'] = self.classify_similarity_level(composite_similarity)
        
        return results

    def normalize_series(self, series: np.ndarray) -> np.ndarray:
        """æ¨™æº–åŒ–æ™‚é–“åºåˆ—"""
        if len(series) == 0:
            return series
        
        # ä½¿ç”¨Z-scoreæ¨™æº–åŒ–
        mean_val = np.mean(series)
        std_val = np.std(series)
        
        if std_val == 0:
            return np.zeros_like(series)
        
        return (series - mean_val) / std_val

    def calculate_volatility(self, df: pd.DataFrame, window: int = 20) -> np.ndarray:
        """è¨ˆç®—æ»¾å‹•æ³¢å‹•ç‡"""
        if len(df) < window:
            window = len(df)
        
        returns = df['Close'].pct_change().dropna()
        volatility = returns.rolling(window=window).std().fillna(0)
        
        return volatility.values

    def calculate_price_volume_relationship(self, df: pd.DataFrame) -> Optional[np.ndarray]:
        """
        è¨ˆç®—åƒ¹é‡é—œä¿‚æŒ‡æ¨™
        
        Returns:
            åƒ¹é‡é—œä¿‚æŒ‡æ¨™æ•¸çµ„ï¼Œæ­£å€¼è¡¨ç¤ºåƒ¹æ¼²é‡å¢ï¼Œè² å€¼è¡¨ç¤ºåƒ¹è·Œé‡å¢
        """
        if 'Volume' not in df.columns or len(df) < 2:
            return None
        
        # è¨ˆç®—åƒ¹æ ¼è®ŠåŒ–
        price_change = df['Close'].pct_change().fillna(0)
        
        # è¨ˆç®—æˆäº¤é‡è®ŠåŒ–
        volume_change = df['Volume'].pct_change().fillna(0)
        
        # åƒ¹é‡é—œä¿‚ = åƒ¹æ ¼è®ŠåŒ– * æˆäº¤é‡è®ŠåŒ–
        # æ­£å€¼: åƒ¹æ¼²é‡å¢æˆ–åƒ¹è·Œé‡æ¸› (å¥åº·)
        # è² å€¼: åƒ¹æ¼²é‡æ¸›æˆ–åƒ¹è·Œé‡å¢ (ä¸å¥åº·)
        price_volume_relationship = price_change * volume_change
        
        return price_volume_relationship.values

    def classify_similarity_level(self, similarity: float) -> str:
        """åˆ†é¡ç›¸ä¼¼æ€§ç­‰ç´š"""
        if similarity >= self.similarity_thresholds['high']:
            return 'HIGH'
        elif similarity >= self.similarity_thresholds['medium']:
            return 'MEDIUM'
        elif similarity >= self.similarity_thresholds['low']:
            return 'LOW'
        else:
            return 'VERY_LOW'

    def analyze_pattern_similarity(self, current_symbol: str, reference_patterns: List[Dict], 
                                 timeframe: str = '1h', days: int = 30) -> Dict:
        """
        åˆ†æç•¶å‰ç¬¦è™Ÿèˆ‡åƒè€ƒæ¨¡å¼çš„ç›¸ä¼¼æ€§
        
        Args:
            current_symbol: ç•¶å‰åˆ†æçš„ç¬¦è™Ÿ
            reference_patterns: åƒè€ƒæ¨¡å¼åˆ—è¡¨
            timeframe: æ™‚é–“æ¡†æ¶
            days: åˆ†æå¤©æ•¸
            
        Returns:
            ç›¸ä¼¼æ€§åˆ†æçµæœ
        """
        print(f"\nğŸ” åˆ†æ {current_symbol} çš„æ¨¡å¼ç›¸ä¼¼æ€§...")
        
        # ç²å–ç•¶å‰æ•¸æ“š
        current_data = self.get_symbol_data(current_symbol, timeframe, days)
        if current_data is None:
            return {'error': f'ç„¡æ³•ç²å– {current_symbol} çš„æ•¸æ“š'}
        
        results = {
            'symbol': current_symbol,
            'timeframe': timeframe,
            'analysis_date': datetime.now().isoformat(),
            'current_data_points': len(current_data),
            'similarities': []
        }
        
        # èˆ‡æ¯å€‹åƒè€ƒæ¨¡å¼æ¯”è¼ƒ
        for i, pattern in enumerate(reference_patterns):
            print(f"   æ¯”è¼ƒæ¨¡å¼ {i+1}/{len(reference_patterns)}: {pattern.get('name', 'Unknown')}")
            
            # ç²å–åƒè€ƒæ•¸æ“š
            ref_data = self.get_reference_data(pattern)
            if ref_data is None:
                continue
            
            # è¨ˆç®—å¤šç¶­åº¦ç›¸ä¼¼æ€§
            similarity_result = self.calculate_multidimensional_similarity(current_data, ref_data)
            
            if 'error' not in similarity_result:
                similarity_info = {
                    'pattern_name': pattern.get('name', 'Unknown'),
                    'pattern_symbol': pattern.get('symbol', 'Unknown'),
                    'pattern_timeframe': pattern.get('timeframe', 'Unknown'),
                    'composite_similarity': similarity_result['composite_similarity'],
                    'similarity_level': similarity_result['similarity_level'],
                    'price_similarity': similarity_result['price_dtw_similarity'],
                    'volume_similarity': similarity_result['volume_dtw_similarity'],
                    'volatility_similarity': similarity_result['volatility_dtw_similarity'],
                    'price_volume_similarity': similarity_result['price_volume_dtw_similarity']
                }
                
                results['similarities'].append(similarity_info)
                
                print(f"      ç¶œåˆç›¸ä¼¼æ€§: {similarity_result['composite_similarity']:.3f} ({similarity_result['similarity_level']})")
        
        # æŒ‰ç›¸ä¼¼æ€§æ’åº
        results['similarities'].sort(key=lambda x: x['composite_similarity'], reverse=True)
        
        # ç”Ÿæˆåˆ†æå ±å‘Š
        self.generate_similarity_report(results)
        
        return results

    def get_symbol_data(self, symbol: str, timeframe: str, days: int) -> Optional[pd.DataFrame]:
        """ç²å–ç¬¦è™Ÿæ•¸æ“š"""
        try:
            end_ts = int(datetime.now().timestamp())
            start_ts = end_ts - (days * 24 * 3600)
            
            success, df = self.downloader.get_data(symbol, start_ts, end_ts, timeframe=timeframe)
            
            if not success or df.empty:
                print(f"âŒ ç„¡æ³•ç²å– {symbol} çš„æ•¸æ“š")
                return None
            
            # æ¨™æº–åŒ–åˆ—å - è™•ç†å¤§å°å¯«å·®ç•°
            column_mapping = {
                'open': 'Open',
                'high': 'High', 
                'low': 'Low',
                'close': 'Close',
                'volume': 'Volume'
            }
            
            # æ‡‰ç”¨åˆ—åæ˜ å°„
            df = df.rename(columns=column_mapping)
            
            # ç¢ºä¿åŒ…å«å¿…è¦çš„åˆ—
            required_columns = ['Open', 'High', 'Low', 'Close']
            if not all(col in df.columns for col in required_columns):
                print(f"âŒ {symbol} æ•¸æ“šç¼ºå°‘å¿…è¦çš„OHLCåˆ—")
                print(f"   å¯ç”¨åˆ—: {list(df.columns)}")
                return None
            
            return df
            
        except Exception as e:
            print(f"âŒ ç²å– {symbol} æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None

    def get_reference_data(self, pattern: Dict) -> Optional[pd.DataFrame]:
        """ç²å–åƒè€ƒæ¨¡å¼æ•¸æ“š"""
        try:
            if 'data' in pattern:
                return pattern['data']
            elif 'symbol' in pattern:
                return self.get_symbol_data(
                    pattern['symbol'], 
                    pattern.get('timeframe', '1h'), 
                    pattern.get('days', 30)
                )
            else:
                print("âŒ åƒè€ƒæ¨¡å¼ç¼ºå°‘å¿…è¦çš„æ•¸æ“šä¿¡æ¯")
                return None
                
        except Exception as e:
            print(f"âŒ ç²å–åƒè€ƒæ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None

    def generate_similarity_report(self, results: Dict):
        """ç”Ÿæˆç›¸ä¼¼æ€§åˆ†æå ±å‘Š"""
        print(f"\nğŸ“Š {results['symbol']} ç›¸ä¼¼æ€§åˆ†æå ±å‘Š")
        print("=" * 60)
        
        if not results['similarities']:
            print("âŒ æ²’æœ‰æ‰¾åˆ°ç›¸ä¼¼çš„æ¨¡å¼")
            return
        
        print(f"âœ… åˆ†æå®Œæˆï¼Œæ‰¾åˆ° {len(results['similarities'])} å€‹ç›¸ä¼¼æ¨¡å¼")
        print(f"ğŸ“ˆ æ•¸æ“šé»æ•¸: {results['current_data_points']}")
        print(f"â° åˆ†ææ™‚é–“: {results['analysis_date']}")
        
        print(f"\nğŸ† ç›¸ä¼¼æ€§æ’å (Top 10):")
        for i, sim in enumerate(results['similarities'][:10], 1):
            level_emoji = {
                'HIGH': 'ğŸ”¥',
                'MEDIUM': 'âš¡',
                'LOW': 'ğŸ’¡',
                'VERY_LOW': 'â„ï¸'
            }.get(sim['similarity_level'], 'â“')
            
            print(f"   {i:2d}. {sim['pattern_symbol']:12s} {level_emoji} {sim['composite_similarity']:.3f}")
            print(f"       åƒ¹æ ¼: {sim['price_similarity']:.3f} | "
                  f"æˆäº¤é‡: {sim['volume_similarity']:.3f} | "
                  f"æ³¢å‹•ç‡: {sim['volatility_similarity']:.3f} | "
                  f"åƒ¹é‡é—œä¿‚: {sim['price_volume_similarity']:.3f}")
        
        # çµ±è¨ˆåˆ†æ
        similarities = [s['composite_similarity'] for s in results['similarities']]
        high_count = len([s for s in similarities if s >= self.similarity_thresholds['high']])
        medium_count = len([s for s in similarities if self.similarity_thresholds['medium'] <= s < self.similarity_thresholds['high']])
        low_count = len([s for s in similarities if self.similarity_thresholds['low'] <= s < self.similarity_thresholds['medium']])
        
        print(f"\nğŸ“ˆ ç›¸ä¼¼æ€§çµ±è¨ˆ:")
        print(f"   é«˜ç›¸ä¼¼æ€§ (â‰¥{self.similarity_thresholds['high']}): {high_count} å€‹")
        print(f"   ä¸­ç­‰ç›¸ä¼¼æ€§ ({self.similarity_thresholds['medium']}-{self.similarity_thresholds['high']}): {medium_count} å€‹")
        print(f"   ä½ç›¸ä¼¼æ€§ ({self.similarity_thresholds['low']}-{self.similarity_thresholds['medium']}): {low_count} å€‹")
        print(f"   å¹³å‡ç›¸ä¼¼æ€§: {np.mean(similarities):.3f}")
        
        # æŠ•è³‡å»ºè­°
        self.generate_investment_advice(results)

    def generate_investment_advice(self, results: Dict):
        """ç”ŸæˆæŠ•è³‡å»ºè­°"""
        print(f"\nğŸ’¡ åŸºæ–¼ç›¸ä¼¼æ€§åˆ†æçš„æŠ•è³‡å»ºè­°:")
        
        if not results['similarities']:
            print("   â“ æ²’æœ‰è¶³å¤ çš„ç›¸ä¼¼æ¨¡å¼é€²è¡Œå»ºè­°")
            return
        
        best_similarity = results['similarities'][0]
        avg_similarity = np.mean([s['composite_similarity'] for s in results['similarities'][:5]])
        
        if best_similarity['composite_similarity'] >= self.similarity_thresholds['high']:
            print(f"   ğŸ”¥ ç™¼ç¾é«˜åº¦ç›¸ä¼¼çš„æ­·å²æ¨¡å¼ï¼")
            print(f"   ğŸ“Š æœ€ä½³åŒ¹é…: {best_similarity['pattern_symbol']} (ç›¸ä¼¼åº¦: {best_similarity['composite_similarity']:.3f})")
            print(f"   ğŸ¯ å»ºè­°: å¯†åˆ‡é—œæ³¨ï¼Œå¯èƒ½é‡ç¾ç›¸ä¼¼èµ°å‹¢")
        elif avg_similarity >= self.similarity_thresholds['medium']:
            print(f"   âš¡ ç™¼ç¾ä¸­ç­‰ç›¸ä¼¼çš„æ­·å²æ¨¡å¼")
            print(f"   ğŸ“Š å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.3f}")
            print(f"   ğŸ¯ å»ºè­°: è¬¹æ…è§€å¯Ÿï¼Œçµåˆå…¶ä»–æŒ‡æ¨™åˆ†æ")
        else:
            print(f"   ğŸ’¡ ç›¸ä¼¼æ€§è¼ƒä½ï¼Œæ­·å²æ¨¡å¼åƒè€ƒåƒ¹å€¼æœ‰é™")
            print(f"   ğŸ¯ å»ºè­°: ä¾è³´åŸºæœ¬é¢å’ŒæŠ€è¡“é¢åˆ†æ")
        
        # åƒ¹é‡é—œä¿‚åˆ†æ
        pv_similarities = [s['price_volume_similarity'] for s in results['similarities'][:5]]
        avg_pv_similarity = np.mean(pv_similarities)
        
        if avg_pv_similarity >= 0.6:
            print(f"   ğŸ“ˆ åƒ¹é‡é—œä¿‚ç›¸ä¼¼æ€§é«˜ ({avg_pv_similarity:.3f})ï¼Œæ¨¡å¼å¯é æ€§è¼ƒå¼·")
        elif avg_pv_similarity >= 0.4:
            print(f"   ğŸ“Š åƒ¹é‡é—œä¿‚ç›¸ä¼¼æ€§ä¸­ç­‰ ({avg_pv_similarity:.3f})ï¼Œéœ€è¦é¡å¤–é©—è­‰")
        else:
            print(f"   ğŸ“‰ åƒ¹é‡é—œä¿‚ç›¸ä¼¼æ€§ä½ ({avg_pv_similarity:.3f})ï¼Œå¯èƒ½å­˜åœ¨è™›å‡ä¿¡è™Ÿ")

    def create_similarity_visualization(self, results: Dict, save_path: str = None) -> Optional[str]:
        """å‰µå»ºç›¸ä¼¼æ€§åˆ†æå¯è¦–åŒ–åœ–è¡¨"""
        if not results['similarities']:
            print("âŒ æ²’æœ‰ç›¸ä¼¼æ€§æ•¸æ“šå¯ä¾›å¯è¦–åŒ–")
            return None
        
        # å‰µå»ºåœ–è¡¨
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))
        
        # 1. ç¶œåˆç›¸ä¼¼æ€§æ’å
        top_similarities = results['similarities'][:10]
        symbols = [s['pattern_symbol'] for s in top_similarities]
        similarities = [s['composite_similarity'] for s in top_similarities]
        
        bars = ax1.barh(symbols, similarities, color=['red' if s < 0.4 else 'orange' if s < 0.6 else 'green' for s in similarities])
        ax1.set_xlabel('ç¶œåˆç›¸ä¼¼æ€§åˆ†æ•¸')
        ax1.set_title(f'{results["symbol"]} - ç¶œåˆç›¸ä¼¼æ€§æ’å')
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for i, (bar, similarity) in enumerate(zip(bars, similarities)):
            ax1.text(similarity + 0.01, i, f'{similarity:.3f}', va='center', fontsize=10)
        
        # 2. å¤šç¶­åº¦ç›¸ä¼¼æ€§é›·é”åœ–
        if len(top_similarities) > 0:
            best_match = top_similarities[0]
            categories = ['åƒ¹æ ¼', 'æˆäº¤é‡', 'æ³¢å‹•ç‡', 'åƒ¹é‡é—œä¿‚']
            values = [
                best_match['price_similarity'],
                best_match['volume_similarity'],
                best_match['volatility_similarity'],
                best_match['price_volume_similarity']
            ]
            
            # å‰µå»ºé›·é”åœ–
            angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()
            values += values[:1]  # é–‰åˆåœ–å½¢
            angles += angles[:1]
            
            ax2.plot(angles, values, 'o-', linewidth=2, color='blue')
            ax2.fill(angles, values, alpha=0.25, color='blue')
            ax2.set_xticks(angles[:-1])
            ax2.set_xticklabels(categories)
            ax2.set_ylim(0, 1)
            ax2.set_title(f'æœ€ä½³åŒ¹é…å¤šç¶­åº¦åˆ†æ\n{best_match["pattern_symbol"]}')
            ax2.grid(True)
        
        # 3. ç›¸ä¼¼æ€§åˆ†ä½ˆç›´æ–¹åœ–
        all_similarities = [s['composite_similarity'] for s in results['similarities']]
        ax3.hist(all_similarities, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
        ax3.axvline(np.mean(all_similarities), color='red', linestyle='--', label=f'å¹³å‡å€¼: {np.mean(all_similarities):.3f}')
        ax3.set_xlabel('ç›¸ä¼¼æ€§åˆ†æ•¸')
        ax3.set_ylabel('é »ç‡')
        ax3.set_title('ç›¸ä¼¼æ€§åˆ†ä½ˆ')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. å„ç¶­åº¦ç›¸ä¼¼æ€§æ¯”è¼ƒ
        dimensions = ['åƒ¹æ ¼', 'æˆäº¤é‡', 'æ³¢å‹•ç‡', 'åƒ¹é‡é—œä¿‚']
        dim_similarities = [
            np.mean([s['price_similarity'] for s in top_similarities[:5]]),
            np.mean([s['volume_similarity'] for s in top_similarities[:5]]),
            np.mean([s['volatility_similarity'] for s in top_similarities[:5]]),
            np.mean([s['price_volume_similarity'] for s in top_similarities[:5]])
        ]
        
        bars = ax4.bar(dimensions, dim_similarities, color=['red', 'green', 'blue', 'orange'])
        ax4.set_ylabel('å¹³å‡ç›¸ä¼¼æ€§åˆ†æ•¸')
        ax4.set_title('å„ç¶­åº¦ç›¸ä¼¼æ€§æ¯”è¼ƒ (Top 5)')
        ax4.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for bar, similarity in zip(bars, dim_similarities):
            ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                    f'{similarity:.3f}', ha='center', va='bottom', fontsize=10)
        
        plt.tight_layout()
        
        # ä¿å­˜åœ–è¡¨
        if save_path is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            save_path = f"similarity_analysis_{results['symbol']}_{timestamp}.png"
        
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š ç›¸ä¼¼æ€§åˆ†æåœ–è¡¨å·²ä¿å­˜: {save_path}")
        
        plt.close()
        return save_path

    def batch_analyze_symbols(self, symbols: List[str], reference_patterns: List[Dict], 
                            timeframe: str = '1h', days: int = 30) -> Dict:
        """æ‰¹é‡åˆ†æå¤šå€‹ç¬¦è™Ÿçš„ç›¸ä¼¼æ€§"""
        print(f"\nğŸš€ é–‹å§‹æ‰¹é‡ç›¸ä¼¼æ€§åˆ†æ...")
        print(f"   ç›®æ¨™ç¬¦è™Ÿ: {len(symbols)} å€‹")
        print(f"   åƒè€ƒæ¨¡å¼: {len(reference_patterns)} å€‹")
        print(f"   æ™‚é–“æ¡†æ¶: {timeframe}")
        print(f"   åˆ†æå¤©æ•¸: {days}")
        
        batch_results = {
            'analysis_date': datetime.now().isoformat(),
            'timeframe': timeframe,
            'days': days,
            'symbols_analyzed': [],
            'total_symbols': len(symbols),
            'reference_patterns': len(reference_patterns),
            'summary': {}
        }
        
        successful_analyses = 0
        failed_analyses = 0
        
        for i, symbol in enumerate(symbols, 1):
            print(f"\n[{i}/{len(symbols)}] åˆ†æ {symbol}...")
            
            try:
                result = self.analyze_pattern_similarity(symbol, reference_patterns, timeframe, days)
                
                if 'error' not in result:
                    batch_results['symbols_analyzed'].append(result)
                    successful_analyses += 1
                else:
                    print(f"âŒ {symbol} åˆ†æå¤±æ•—: {result['error']}")
                    failed_analyses += 1
                    
            except Exception as e:
                print(f"âŒ {symbol} åˆ†ææ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                failed_analyses += 1
        
        # ç”Ÿæˆæ‰¹é‡åˆ†ææ‘˜è¦
        batch_results['summary'] = {
            'successful_analyses': successful_analyses,
            'failed_analyses': failed_analyses,
            'success_rate': successful_analyses / len(symbols) if symbols else 0
        }
        
        # ç”Ÿæˆæ‰¹é‡å ±å‘Š
        self.generate_batch_report(batch_results)
        
        return batch_results

    def generate_batch_report(self, batch_results: Dict):
        """ç”Ÿæˆæ‰¹é‡åˆ†æå ±å‘Š"""
        print(f"\nğŸ“Š æ‰¹é‡ç›¸ä¼¼æ€§åˆ†æå ±å‘Š")
        print("=" * 80)
        
        summary = batch_results['summary']
        print(f"âœ… æˆåŠŸåˆ†æ: {summary['successful_analyses']} å€‹ç¬¦è™Ÿ")
        print(f"âŒ å¤±æ•—åˆ†æ: {summary['failed_analyses']} å€‹ç¬¦è™Ÿ")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {summary['success_rate']:.1%}")
        
        if not batch_results['symbols_analyzed']:
            print("âŒ æ²’æœ‰æˆåŠŸçš„åˆ†æçµæœ")
            return
        
        # æ‰¾å‡ºæœ€ä½³åŒ¹é…
        best_matches = []
        for result in batch_results['symbols_analyzed']:
            if result['similarities']:
                best_sim = result['similarities'][0]
                best_matches.append({
                    'symbol': result['symbol'],
                    'best_match': best_sim['pattern_symbol'],
                    'similarity': best_sim['composite_similarity'],
                    'level': best_sim['similarity_level']
                })
        
        # æŒ‰ç›¸ä¼¼æ€§æ’åº
        best_matches.sort(key=lambda x: x['similarity'], reverse=True)
        
        print(f"\nğŸ† æœ€ä½³ç›¸ä¼¼æ€§åŒ¹é… (Top 15):")
        for i, match in enumerate(best_matches[:15], 1):
            level_emoji = {
                'HIGH': 'ğŸ”¥',
                'MEDIUM': 'âš¡',
                'LOW': 'ğŸ’¡',
                'VERY_LOW': 'â„ï¸'
            }.get(match['level'], 'â“')
            
            print(f"   {i:2d}. {match['symbol']:12s} â†’ {match['best_match']:12s} "
                  f"{level_emoji} {match['similarity']:.3f}")
        
        # çµ±è¨ˆç›¸ä¼¼æ€§ç­‰ç´šåˆ†ä½ˆ
        level_counts = {'HIGH': 0, 'MEDIUM': 0, 'LOW': 0, 'VERY_LOW': 0}
        for match in best_matches:
            level_counts[match['level']] += 1
        
        print(f"\nğŸ“ˆ ç›¸ä¼¼æ€§ç­‰ç´šåˆ†ä½ˆ:")
        for level, count in level_counts.items():
            percentage = count / len(best_matches) * 100 if best_matches else 0
            emoji = {'HIGH': 'ğŸ”¥', 'MEDIUM': 'âš¡', 'LOW': 'ğŸ’¡', 'VERY_LOW': 'â„ï¸'}[level]
            print(f"   {emoji} {level:8s}: {count:3d} å€‹ ({percentage:5.1f}%)")
        
        # å¹³å‡ç›¸ä¼¼æ€§
        avg_similarity = np.mean([m['similarity'] for m in best_matches]) if best_matches else 0
        print(f"\nğŸ“Š å¹³å‡ç›¸ä¼¼æ€§: {avg_similarity:.3f}")
        
        # æŠ•è³‡å»ºè­°æ‘˜è¦
        high_similarity_count = level_counts['HIGH']
        if high_similarity_count > 0:
            print(f"\nğŸ’¡ æŠ•è³‡å»ºè­°æ‘˜è¦:")
            print(f"   ğŸ”¥ ç™¼ç¾ {high_similarity_count} å€‹é«˜ç›¸ä¼¼æ€§ç¬¦è™Ÿï¼Œå€¼å¾—é‡é»é—œæ³¨")
            print(f"   ğŸ“Š å»ºè­°å„ªå…ˆåˆ†æé€™äº›ç¬¦è™Ÿçš„æ­·å²æ¨¡å¼å’Œæœªä¾†èµ°å‹¢")
        elif level_counts['MEDIUM'] > 0:
            print(f"\nğŸ’¡ æŠ•è³‡å»ºè­°æ‘˜è¦:")
            print(f"   âš¡ ç™¼ç¾ {level_counts['MEDIUM']} å€‹ä¸­ç­‰ç›¸ä¼¼æ€§ç¬¦è™Ÿ")
            print(f"   ğŸ“Š å»ºè­°çµåˆå…¶ä»–æŠ€è¡“æŒ‡æ¨™é€²è¡Œç¶œåˆåˆ†æ")
        else:
            print(f"\nğŸ’¡ æŠ•è³‡å»ºè­°æ‘˜è¦:")
            print(f"   ğŸ’¡ æ•´é«”ç›¸ä¼¼æ€§è¼ƒä½ï¼Œå»ºè­°ä¾è³´åŸºæœ¬é¢å’ŒæŠ€è¡“é¢åˆ†æ")

# ä½¿ç”¨ç¤ºä¾‹å’Œæ¸¬è©¦å‡½æ•¸
def create_sample_reference_patterns():
    """å‰µå»ºç¤ºä¾‹åƒè€ƒæ¨¡å¼"""
    patterns = [
        {
            'name': 'BTC_Bull_Run_2024',
            'symbol': 'BTCUSDT',
            'timeframe': '1h',
            'days': 30,
            'description': 'BTC 2024å¹´ç‰›å¸‚æ¨¡å¼'
        },
        {
            'name': 'ETH_Breakout_Pattern',
            'symbol': 'ETHUSDT',
            'timeframe': '1h',
            'days': 30,
            'description': 'ETH çªç ´æ¨¡å¼'
        },
        {
            'name': 'SOL_Recovery_Pattern',
            'symbol': 'SOLUSDT',
            'timeframe': '1h',
            'days': 30,
            'description': 'SOL å¾©ç”¦æ¨¡å¼'
        }
    ]
    
    return patterns

def test_enhanced_similarity_analyzer():
    """æ¸¬è©¦å¢å¼·ç‰ˆç›¸ä¼¼æ€§åˆ†æå™¨"""
    print("ğŸ§ª æ¸¬è©¦å¢å¼·ç‰ˆç›¸ä¼¼æ€§åˆ†æå™¨...")
    
    # å‰µå»ºåˆ†æå™¨
    analyzer = EnhancedSimilarityAnalyzer()
    
    # å‰µå»ºåƒè€ƒæ¨¡å¼
    reference_patterns = create_sample_reference_patterns()
    
    # æ¸¬è©¦å–®å€‹ç¬¦è™Ÿåˆ†æ
    test_symbol = 'ADAUSDT'
    result = analyzer.analyze_pattern_similarity(test_symbol, reference_patterns)
    
    # å‰µå»ºå¯è¦–åŒ–
    if 'error' not in result:
        analyzer.create_similarity_visualization(result)
    
    print("âœ… æ¸¬è©¦å®Œæˆ")

if __name__ == "__main__":
    test_enhanced_similarity_analyzer() 